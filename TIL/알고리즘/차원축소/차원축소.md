---
sticker: emoji//1f419
---
* 매우 많은 피처로 구성된 다차원 데이터 세트의 차원을 축소해 새로운 차원의 데이터 세트를 생성
	일반적으로 차원축소는 피처 선택과 피처 추출로 나눌 수 있음.
	
	피처 선택
		특정 피처에 종속성이 강한 불필요한 피처는 제거
		장점은 피처의 해석이 용이함.
		단점은 피처간 상관관계를 고려하기 어렵다는 점.
	피처 추출
		기존 피처를 저차원의 중요 피처로 압축해서 추출하는 것
		기존의 피처와는 완전이 다른 값이 됨.
		장점은 피처 간 상관관계를 고려하기 용이함.
		단점은 추출된 변수의 해석이 어려움.


### PCA
가장 대표적인 차원 축소 기법
_데이터들의 공분산 행렬에 대한 특이값 분해(SVD) 로 볼 수 있음._
PCA 기법의 핵심은 **데이터를 축에 사영했을 때 가장 높은 분산을 가지는 데이터의 축을 찾아 그 축으로 차원을 축소**  해당 축을 **주성분**이라고 한다.

높은 분산을 가지는 축을 찾는 이유 => **정보의 손실을 최소화** 하기 위해서

_분산이 크다는 것 -> 원래 데이터의 분포를 잘 설명할 수 있다는 것을 뜻함._

![[Pasted image 20231227154804.png]]

데이터를 좌측 축에 사영시키는 것이 데이터의 원래 분포를 잘 설명한다고 볼 수 있음.

PCA 는 제일 먼저 **가장 큰 분산을 기반으로 첫번째 축을 생성**, 두번째 축으로 **이 벡터 축에 직각이 되는 벡터를 축**으로 함. 그리고 세번째 축은 **다시 두번째 축과 직각이 되는 벡터를 축으로 설정**하는 방식으로 축을 생성함.

이렇게 생성된 벡터 축에 원본 데이터를 투영하면 벡터축의 갯수만큼 차원으로 원본 데이터가 차원 축소됨.

#### 왜 직각?
첫 번째 축에 데이터를 사영했는데 같은 점으로 겹쳐서 사영된 데이터가 10개 있으면 첫번째 축으로는 데이터를 모두 설명할 수 없게 됨. -> 다른축이 필요

이때 직각인 축으로 사영하게 되면 겹쳤던 10개의 데이터는 절대 같은 점으로 사영 될 수 없고 두번째 축이 이 10개의 데이터를 설명할 수 있음.

이런 원리로 계속 직교하는 축을 다음 축으로 설정하는 방식으로 축을 생성하게 됨.

_직각으로 만드는 이유  [주성분분석](obsidian://open?vault=TIL_yeonsang&file=TIL%2F%EC%88%98%ED%95%99%2F%EC%A3%BC%EC%84%B1%EB%B6%84%20%EB%B6%84%EC%84%9D)_


#### 축을 찾는 방법
입력 데이터의 **공분산 행렬을 고유값 분해** 했을 때 구해진 **고유벡터** 가 **PCA의 주성분 벡터** 로써 입력 데이터의 **분산이 큰 방향** 을 나타냄.

_[공분산](https://kh-mo.github.io/notation/2021/01/02/covariance/) 관련 지식_

**고유값** 이 **고유벡터의 크기를 나타내고 입력 데이터의 분산**을 나타냄.

공분산은 두 변수 간의 변동을 의미하고 고유 벡터는 n x n 행렬 A에 대해서 $Ax = ax$ (a는 스칼라값) 을 만족하는 x를 고유벡터라고 함.

이때, a는 고유값.

**공분산 행렬은 정방행렬이며 대칭행렬**인데, **대칭행렬은 항상 고유벡터를 직교행렬로, 고유값을 정방 행렬로 대각화할 수 있는 특성을 가지고 있음**.

공분산 행렬을 A라고 했을 때,
$$
\begin{align*}
A = \begin{pmatrix}
v_1&v_2&\dots&v_n
\end{pmatrix}
\begin{pmatrix}
\lambda_1&0&0&0\\
0&\lambda_2&0&0\\
0&0&\ddots&\vdots\\
0&0&\dots&\lambda_n
\end{pmatrix}
\begin{pmatrix}
v1&v2&\dots&v_n
\end{pmatrix}^{-1}
\end{align*}
$$
고유값을 분리할 수 있음.

$v_1,v_2\dots v_n$ 이 고유 벡터, $\lambda_1,\lambda_2,\dots \lambda_n$ 이 고유값에 해당.



----
정리
입력 데이터의 공분산 행렬이 고유벡터와 고유값으로 분해될 수 있으며, 이렇게 분해된 고유벡터를 이용해 입력 데이터를 선형 변환하는 방식이 PCA 라는 것.

순서
1. 입력 데이터 세트의 공분산 행렬을 구함.
2. 공분산 행렬을 고유값 분해해서 고유벡터와 고유값을 구함.
3. 고유값이 가장 큰 순으로 K개 (변환 차수) 만큼 고유벡터를 추출함
4. 고유벡터에 입력 데이터를 선형 변환함.
------------------

# LDA 

### LDA 란?
선형 판별 분석법으로 PCA와 비슷.

PCA와 비슷하지만 지도 학습의 분류에 사용하기 쉽게 개별 클래스를 분별할 수 있는 기준을 최대한 유지하면서 축소함.

PCA는 데이터의 변동성이 가장 큰 축을 찾고, LDA는 데이터의 결정 클래스를 최대한 분리할 수 있는 축을 찾는 것

LDA는 특정 공간 내 클래스를 최대로 분리할 수 있는 축을 찾기 위해 클래스간 분산과 클래스 내부 분산의 비율을 최대화 하는 방식으로 차원을 축소함.
➡️ 같은 클래스 끼리는 뭉쳐있고, 다른 클래스와의 거리를 최대화 하게 하여 분류하기 쉬운 차원으로 축소
클래스간 분산을 최대화, 클래스 내부 분산은 최대한 작게
	ex) 특정 공간 => 뉴스기사,        클래스 => 정치,경제,스포츠 등 주제들

### LDA 구하기
1. 분산 행렬을 구함. (클래스 내부와 클래스 간)
	 ➡️ 두개의 행렬은 입력 데이터의 결정 값 클래스 별로 개별 [종속변수](obsidian://open?vault=TIL_yeonsang&file=TIL%2F%EB%8D%B0%EC%9D%B4%ED%84%B0%20%EB%B6%84%EC%84%9D%2F%ED%94%BC%EC%B2%98)의 평균 벡터를 기반으로 한다.
	 종속변수: 결과가 되는 컬럼  
		 예측하고자 하는 변수, 클래스 레이블로서의 종속변수는 데이터 포인트가 속한 그룹 또는 카테고리를 나타냄.
		 ex) 환자 데이터를 '아픈' or '건강한'으로 분류하는 경우에 '아픈', '건강한' 이라는 레이블이 종속변수가 됨.
2. 두 행렬을 고유 벡터로 분리
	$$ 
	S_W^T = 
	\begin{pmatrix}
	e_1&\dots&e_n
	\end{pmatrix}
	\begin{pmatrix}
	\lambda_1&\dots&0\\
	\vdots&\ddots&\vdots\\
	0&\dots&\lambda_n
	\end{pmatrix}
	\begin{pmatrix}
	e_1^T\\
	\vdots\\
	e_n^T
	\end{pmatrix}
	 $$
	 
3. 고유값이 가장 큰 순으로 K 개 (LDA 에서 정한 차수) 를 추출
4. 고유값이 가장 큰 순으로 추출된 고유벡터를 이용해 새롭게 데이터를 변환


----
정리

클래스, 내부 클래스를 나눠서 차원축소를 함.

특정 공간은 하나의 주제를 나타내고, 클래스는 공간의 카테고리 등을 의미한다.

클래스들의 분산은 크게, 내부 클래스 안에 있는 데이터들은 분산을 적게 해서 알아보기 쉽게 한다.

---------


test